{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 : Markov Decision Processes (David Silver)\n",
    "\n",
    "---\n",
    "\n",
    "## List\n",
    "\n",
    "### 1. Markov Processes\n",
    "\n",
    "### 2. Markov Reward Processes\n",
    "\n",
    "### 3. Markov Decision Processes\n",
    "\n",
    "### 4. Extensions to MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to MDPs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Markov decision processes(MDP)** formally describe an environment for reinforcement learning\n",
    "- Where the environment is $\\color{red}{\\text{fully observable}}$\n",
    "- i.e. The current state completely characterises the process \n",
    "- Almost all RL problems can be formalised as MDPs, e.g. \n",
    "    - Optimal control primarily deals with continuous MDPs \n",
    "    - Partially observable problems can be converted into MDPs \n",
    "    - Bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property\n",
    "---\n",
    "\n",
    "### \"The future is independent of the past given the present”\n",
    "\n",
    "### Definition\n",
    "> A state $S_t$ is **Markov** if and only if\n",
    "> $$ \n",
    "\\mathbb{P}[S_{t+1} | S_t | = \\mathbb{P}[S_{t+1} | S_1, \\ldots, S_t]\n",
    "$$\n",
    "\n",
    "- The state captures all relevant information from the history \n",
    "- Once the state is known, the history may be thrown away \n",
    "- i.e. The state is a suﬃcient statistic of the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Transition Matrix\n",
    "---\n",
    "For a Markov state $s$ and successor state $s'$, the state *transition probability* is deﬁned by \n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'| S_t = s]\n",
    "$$\n",
    "\n",
    "State transition matrix $\\mathcal{P}$ deﬁnes transition probabilities from all states $s$ to all successor states $s'$,\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\text{from} \\begin{bmatrix}\n",
    "    \\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\\n",
    "    \\vdots &  &  \\\\\n",
    "    \\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each row of the matrix sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Process\n",
    "\n",
    "---\n",
    "A $\\color{blue}{\\text{Markov process}}$ is a memoryless random process, i.e. a sequence of random states $S_1$,$S_2$,... with the **Markov property**.\n",
    "\n",
    "\n",
    "### Definition\n",
    "> A Markov Process (or Markov Chain) is a tuple $<\\mathcal{S},\\mathcal{P}>$ \n",
    "> - $\\mathcal{S}$ is a (finite) set of states\n",
    "> - $\\mathcal{P}$ is a state transition probability matrix,\n",
    "> $$\n",
    "\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'| S_t = s]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Student Markov Chain\n",
    "---\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-1.PNG?raw=true\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Markov Chain Episodes\n",
    "---\n",
    "\n",
    "Sample $\\color{red}{\\text{episodes}}$ for Student Markov Chain starting from $S_1$ = $C_1$\n",
    "\n",
    "$$\n",
    "S_1,S_2,...,S_T\n",
    "$$\n",
    "\n",
    "- C1 C2 C3 Pass Sleep\n",
    "- C1 FB FB C1 C2 Sleep\n",
    "- C1 C2 C3 Pub C2 C3 Pass Sleep\n",
    "- C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : Student Markov Chain Transition Matrix\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-2.PNG?raw=true\" width=\"950\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Reward Process\n",
    "---\n",
    "#### A Markov reward process is a Markov chain with values.\n",
    "\n",
    "### Definition\n",
    "> A **Markov Reward Process** is a tuple $<\\mathcal{S},\\mathcal{P}, \\color{red}{\\mathcal{R}}, \\color{red}{\\mathcal{\\gamma}}>$ \n",
    "> - $\\mathcal{S}$ is a (finite) set of states\n",
    "> - $\\mathcal{P}$ is a state transition probability matrix,  \n",
    "> $\n",
    "\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'| S_t = s]\n",
    "$\n",
    "> - $\\color{red}{\\mathcal{R}}$ is a reward function, $\\color{red}{\\mathcal{R}_s}$ = $\\mathbb{E}[R_{t+1} | S_t = s]$\n",
    "> - $\\color{red}{\\gamma}$ is a discount factor, $\\color{red}{\\gamma} \\in [0,1]$\n",
    "\n",
    "### Example : Student MRP\n",
    "---\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-3.PNG?raw=true\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "---\n",
    "\n",
    "### Definition\n",
    "> The $\\bbox[yellow]{\\text{return } G_t} $ is $\\color{blue}{\\text{the total discounted reward from time-step t}}.$\n",
    "> $$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots =\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}\n",
    "$$\n",
    "\n",
    "- The discount $\\gamma \\in [0,1]$ is the present value of future rewards.\n",
    "- The value of receiving reward $R$ after $k +1$ time-steps is $\\gamma^k$$R$.\n",
    "- This values immediate reward above delayed reward. \n",
    "    - $\\gamma$ close to 0 leads to ”myopic” evaluation \n",
    "    - $\\gamma$ close to 1 leads to ”far-sighted” evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why discount?\n",
    "---\n",
    "\n",
    "#### Most Markov reward and decision processes are discounted. Why?\n",
    "- Mathematically convenient to discount rewards\n",
    "- Avoids inﬁnite returns in cyclic Markov processes\n",
    "- Uncertainty about the future may not be fully represented\n",
    "- If the reward is ﬁnancial, immediate rewards may earn more interest than delayed rewards \n",
    "- Animal/human behaviour shows preference for immediate reward \n",
    "- It is sometimes possible to use undiscounted Markov reward processes (i.e.$\\gamma$ = 1), e.g. if all sequences terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function\n",
    "---\n",
    "#### The $\\bbox[yellow]{\\text{value function } v(s)} $ gives the long-term value of state $s$.\n",
    "\n",
    "### Definition\n",
    "> The state value function $v(s)$ of an MRP is the expected return starting from state $s$ \n",
    "> \n",
    "> $$\n",
    "v_s = \\mathbb{E}[G_t | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Student MRP Returns\n",
    "---\n",
    "\n",
    "Sample $\\color{red}{\\text{returns }}$ for Student MRP:  \n",
    "Starting from $S_1$ = $C_1$ with $\\gamma$= $\\frac{1}{2}$ \n",
    "\n",
    "$$\n",
    "G_1 = R_2 + \\gamma R_3 + \\ldots + \\gamma^{T-2} R_T\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C1 C2 C3 Pass Sleep\n",
    "    - $v_1$ = - 2 - 2 x $\\frac{1}{2}$ - 2 x $\\frac{1}{4}$ + 10 x $\\frac{1}{8}$ = - 2.25\n",
    "- C1 FB FB C1 C2 Sleep\n",
    "    - $v_1$ = - 2 - 1 x $\\frac{1}{2}$ - 1 x $\\frac{1}{4}$ - 2 x $\\frac{1}{8}$ - 2 x $\\frac{1}{16}$ = - 3.125\n",
    "- C1 C2 C3 Pub C2 C3 Pass Sleep\n",
    "- C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : State-Value Function for Student MRP (1)\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-4.PNG?raw=true\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : State-Value Function for Student MRP (2)\n",
    "---\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-5.PNG?raw=true\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.015"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 0.9\n",
    "-2 + 0.9*0.5*r + -7.6*0.5*r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : State-Value Function for Student MRP (3)\n",
    "---\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-6.PNG?raw=true\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 + 0.6*10 + 0.4*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for MRPs\n",
    "---\n",
    "\n",
    "#### The $\\bbox[yellow]{\\text{value function}} $ can be decomposed into two parts:\n",
    "- immediate reward $R_{t+1}$ \n",
    "- discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "$$\\begin{align}\n",
    "v(s) &= \\mathbb{E}[G_t | S_t = s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1} + \\gamma R_{r+2} + \\gamma^2 R_{r+3} + \\ldots | S_t = s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1} + \\gamma (R_{r+2} + \\gamma R_{r+3} + \\ldots ) | S_t = s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) | S_t = s]     \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for MRPs (2)\n",
    "---\n",
    "$$\n",
    "v(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s]\n",
    "$$\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-7.PNG?raw=true\" width=\"800\">\n",
    "\n",
    "\n",
    "$$\n",
    "v(s) = \\mathcal{R}_s + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'} v(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bellman Equation for Student MRP\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-8.PNG?raw=true\" width=\"800\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation in Matrix Form\n",
    "---\n",
    "\n",
    "The Bellman equation can be expressed concisely using matrices,\n",
    "\n",
    "$$\n",
    "\\text{v} = \\mathcal{R} + \\gamma \\mathcal{P}\\text{v}\n",
    "$$\n",
    "\n",
    "where v is a column vector with one entry per state \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    v(1) \\\\\n",
    "    \\vdots \\\\\n",
    "    v(n) \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    \\mathcal{R}_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathcal{R}_n\n",
    "\\end{bmatrix} + \\gamma\n",
    "\\begin{bmatrix}\n",
    "    \\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\\n",
    "    \\vdots &  &  \\\\\n",
    "    \\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "    v(1) \\\\\n",
    "    \\vdots \\\\\n",
    "    v(n) \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Bellman Equation\n",
    "---\n",
    "\n",
    "- The Bellman equation is a linear equation\n",
    "- It can be solved directly:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{v} &= \\mathcal{R} + \\gamma \\mathcal{P}\\text{v} \\\\\n",
    "(I - \\gamma \\mathcal{P}) \\text{v} &= \\mathcal{R} \\\\\n",
    "\\text{v} &= (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R}\n",
    "\\end{align}$$\n",
    "\n",
    "- Computational complexity is O(n3) for n states\n",
    "- Direct solution only possible for small MRPs \n",
    "- There are many iterative methods for large MRPs, e.g. \n",
    "    - Dynamic programming \n",
    "    - Monte-Carlo evaluation \n",
    "    - Temporal-Diﬀerence learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7999999999999998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 + 0.5*2.7 + -2.3*0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
