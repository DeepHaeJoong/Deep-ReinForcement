{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "---\n",
    "\n",
    "- A reward $R_t$ is a scalar feedback signal.\n",
    "- Indicates how well agent is doing at step t\n",
    "- The Agent's Job is to maximise cumumlative reward\n",
    "\n",
    "Reinforcement learning is based on the $\\color{red}{\\text{rewrad hypothesis}}$.\n",
    "\n",
    ">### Definition (Reward Hypotheis)\n",
    ">\n",
    "> All goals can be described by the maximisation of expected cummulative reward.\n",
    "\n",
    "Do yoy agree with this statement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Decision Making\n",
    "\n",
    "---\n",
    "\n",
    "#### - Goal: select actions to maximise total future reward \n",
    "\n",
    "#### - Actions may have long term consequences\n",
    "\n",
    "#### - Reward may be delayed \n",
    "\n",
    "#### - It may be better to sacriﬁce immediate reward to gain more long-term reward\n",
    "\n",
    "#### Examples: \n",
    "\n",
    "- A ﬁnancial investment (may take months to mature) \n",
    "- Refuelling a helicopter (might prevent a crash in several hours)\n",
    "- Blocking opponent moves (might help winning chances many moves from now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and State\n",
    "---\n",
    "\n",
    "**중요도 : History < State**\n",
    "\n",
    "- $\\color{red}{\\text{State}}$ is the information used to determine what happen next\n",
    "- Formally, state is a function of the history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviromnet State\n",
    "\n",
    "### Agent State\n",
    "\n",
    "내가 액션을 할때, 참고하는 숫자들\n",
    "\n",
    "- It can be any function of history (확실한 과거 정보를 포함하고 있는 정보)\n",
    "\n",
    "$$\n",
    "S_{t}^{\\text{a}} = f(H_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information State\n",
    "---\n",
    "\n",
    "An $\\color{red}{\\text{information state}}$(a.k.a $\\color{red}{\\text{Markov state}}$) contains all useful information from the history.\n",
    "\n",
    "> ### Definition\n",
    "> A state $S_t$ is $\\color{red}{\\text{Markov}}$ if and only if\n",
    "> $$ \n",
    "\\mathbb{P}[S_{t+1} | S_t | = \\mathbb{P}[S_{t+1} | S_1, \\ldots, S_t]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, \"현재 상태를 알고 있는 순간, 과거의 상태를 알 필요가 없다\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\color{blue}{\\text{“The future is independent of the past given the present”}}$\n",
    "\n",
    "$$\n",
    "H_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\n",
    "$$\n",
    "\n",
    "- Once the state is known, the history may be thrown away \n",
    "- i.e. The state is a suﬃcient statistic of the future \n",
    "- The environment state $S_{t}^{e}$ is Markov \n",
    "- The history $H_{t}$ is Markov\n",
    "\n",
    "**의견 : RL은 대부분 Markov원리(과거와는 독립적)를 이용하여 문제를 해결한다?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Components of an RL Agent\n",
    "\n",
    "#### An RL agent may include one or more of these components: \n",
    "- **Policy**: agent’s behaviour function \n",
    "- **Value function**: how good is each state and/or action \n",
    "- **Model**: agent’s representation of the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "- A $\\color{red}{policy}$ is the agent’s behaviour : Agent의 행동\n",
    "- It is a map from state to action, e.g. : State를 넣어주면 Action을 내보냄\n",
    "    - Deterministic policy: $\\text{a} = \\pi(s)$ \n",
    "    - Stochastic policy: $\\pi(\\text{a}|s) = P[A_t = \\text{a}|S_t = s]$ : 확률에 따른 Action을 내보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function\n",
    "\n",
    "- Value function is a prediction of future reward : 상황이 얼마나 좋은지 나타냄(미래 reward를 합산)\n",
    "- Used to evaluate the goodness/badness of states \n",
    "- And therefore to select between actions, e.g.\n",
    "$$\n",
    "\\text{v}_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma R_{t+2} + \t\\gamma^2 R_{t+3} + \\ldots| S_t = s ] \n",
    "$$\n",
    "\n",
    "위의 수식에 익숙해져야 합니다.  \n",
    "내가 $s$로부터 $\\pi$를 따라서 끝까지 나오는 기댓값\n",
    "\n",
    "- Enviroment에 대한 확률\n",
    "- Action에 대한 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q^{*}(s, \\text{a})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "- A model predicts what the environment will do next : 환경이 어떻게 변할지 예측 [두 가지로 나뉨]\n",
    "- $\\mathcal{P}$ predicts the next state [다음 상태를 예측]\n",
    "- $\\mathcal{R}$ predicts the next (immediate) reward, [다음 (즉각적인) 보상을 예측]e.g. \n",
    "\n",
    "$$\n",
    "\\mathcal{P}^{\\text{a}}_{ss'} = \\mathbb{P}_\\pi[S_{t+1} = s'| S_t = s, A_t = \\text{a} ] \\\\\n",
    "\\mathcal{R}^{\\text{a}}_{s} = \\mathbb{E}_\\pi[R_{t+1} | S_t = s, A_t = \\text{a} ]\n",
    "$$\n",
    "\n",
    "강화학습은 모델을 안쓰면 : model free, 모델을 사용하면 : model based 로 나뉨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Example\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%201.PNG?raw=true\" width=\"800\">\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202.PNG?raw=true\" width=\"800\">\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%203.PNG?raw=true\" width=\"800\">\n",
    "\n",
    "> 내가 앞으로 받을 Reward의 총합, value function은 policy가 있어야하며, policy에 의존하고, 위의 그림은 Optimal Policy를 따랐을 경우의 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Example : Model\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%204.PNG?raw=true\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model은 Agent가 생각하는 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing RL Agents (1)\n",
    "\n",
    "#### Value Based\n",
    "- No Policy(Implicit)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Control\n",
    "\n",
    "### Prediction : evaluate the future\n",
    "\n",
    "- Given a policy : Value function을 학습!\n",
    "\n",
    "### Control : optimise the future\n",
    "\n",
    "- Find the best policy : Policy를 찾는 문제!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
