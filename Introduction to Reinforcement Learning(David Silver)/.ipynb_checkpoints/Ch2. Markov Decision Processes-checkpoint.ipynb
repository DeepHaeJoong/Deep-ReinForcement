{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 : Markov Decision Processes (David Silver)\n",
    "\n",
    "---\n",
    "\n",
    "## List\n",
    "\n",
    "### 1. Markov Processes\n",
    "\n",
    "### 2. Markov Reward Processes\n",
    "\n",
    "### 3. Markov Decision Processes\n",
    "\n",
    "### 4. Extensions to MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to MDPs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Markov decision processes(MDP)** formally describe an environment for reinforcement learning\n",
    "- Where the environment is $\\color{red}{\\text{fully observable}}$\n",
    "- i.e. The current state completely characterises the process \n",
    "- Almost all RL problems can be formalised as MDPs, e.g. \n",
    "    - Optimal control primarily deals with continuous MDPs \n",
    "    - Partially observable problems can be converted into MDPs \n",
    "    - Bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property\n",
    "---\n",
    "\n",
    "### \"The future is independent of the past given the present”\n",
    "\n",
    "### Definition\n",
    "> A state $S_t$ is **Markov** if and only if\n",
    "> $$ \n",
    "\\mathbb{P}[S_{t+1} | S_t | = \\mathbb{P}[S_{t+1} | S_1, \\ldots, S_t]\n",
    "$$\n",
    "\n",
    "- The state captures all relevant information from the history \n",
    "- Once the state is known, the history may be thrown away \n",
    "- i.e. The state is a suﬃcient statistic of the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Transition Matrix\n",
    "---\n",
    "For a Markov state $s$ and successor state $s'$, the state *transition probability* is deﬁned by \n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'| S_t = s]\n",
    "$$\n",
    "\n",
    "State transition matrix $\\mathcal{P}$ deﬁnes transition probabilities from all states $s$ to all successor states $s'$,\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\text{from} \\begin{bmatrix}\n",
    "    \\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\\n",
    "    \\vdots &  &  \\\\\n",
    "    \\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each row of the matrix sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Process\n",
    "\n",
    "---\n",
    "A $\\color{blue}{\\text{Markov process}}$ is a memoryless random process, i.e. a sequence of random states $S_1$,$S_2$,... with the **Markov property**.\n",
    "\n",
    "\n",
    "### Definition\n",
    "> A Markov Process (or Markov Chain) is a tuple $<\\mathcal{S},\\mathcal{P}>$ \n",
    "> - $\\mathcal{S}$ is a (finite) set of states\n",
    "> - $\\mathcal{P}$ is a state transition probability matrix,\n",
    "> $$\n",
    "\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'| S_t = s]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Student Markov Chain\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/Introduction%20to%20Reinforcement%20Learning(David%20Silver)/PNG/Figure%202-1.PNG?raw=true\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Markov Chain Episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample $\\color{red}{\\text{episodes}}$ for Student Markov Chain starting from $S_1$ = $C_1$\n",
    "\n",
    "$$\n",
    "S_1,S_2,...,S_T\n",
    "$$\n",
    "\n",
    "- C1 C2 C3 Pass Sleep\n",
    "- C1 FB FB C1 C2 Sleep\n",
    "- C1 C2 C3 Pub C2 C3 Pass Sleep\n",
    "- C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
