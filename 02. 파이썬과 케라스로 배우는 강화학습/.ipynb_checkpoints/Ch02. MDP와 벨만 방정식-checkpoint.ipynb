{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP와 벨만 방정식\n",
    "\n",
    "강화학습이 결국 어떠한 방정식을 풀어내는 방법이라면 그 방정식이 무엇인지 아는것이 중요하다.\n",
    "\n",
    "강화학습을 공부하기에 앞서 순차적 행동 결정 문제에 대해 살펴보자. \n",
    "\n",
    "#### 순차적 행동 결정 문제는 MDP로 정의\n",
    "\n",
    "e.g ) 순차적 행동 결정 문제의 간단한 예시 : 그리드월드를 통해 MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MDP(Markov Decision Process)\n",
    "---\n",
    "> **순차적으로 행동을 계속 결정해야 하는 문제를 푸는 것**\n",
    "\n",
    "## 1.1 MDP의 구성요소\n",
    "\n",
    "**1. 상태**  \n",
    "**2. 행동**  \n",
    "**3. 보상 함수**   \n",
    "**4. 상태 변환 확률 (State Transition Probability)**  \n",
    "**5. 감가율 (Discount Factor)**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 상태 (State)\n",
    "\n",
    "#### <정의> \n",
    "\n",
    "$\\mathbf{S}$는 에이전트가 관찰 가능한 상태의 집합이다.\n",
    "\n",
    "상태란? \n",
    "\n",
    "**\"자신의 상황에 대한 관찰\"**이 상태에 대한 가장 정확한 표현이다.\n",
    "\n",
    "로봇과 같은 실제 세상에서의 에이전트에게 상태는 **센서 값**이 될 것이다. 하지만 이 책에서와 같이 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해줘야 한다.\n",
    "\n",
    "\n",
    "하지만 이 책에서와 같이 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해줘야 합니다. 이때 \"내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?:라는 질문을 해야 한다.\n",
    "\n",
    "그리드월드에서 에이전트가 학습할 때는 상태 공간이 워낙 작으므로 상태의 정의 문제가 중요하지 않을 수도 있다. 애초에 문제가 작기 때문에 학습이 잘 안된다는 문제를 발견하기 어려운 것입니다. 하지만 방대하고 복잡한 상태 안에서 학습하는 에이전트를 구현할 때는 많이 고민해야 할 문제이다.\n",
    "\n",
    "그리드월드에서 상태의 개수는 유한하다. 만약 그리드 월드에 상태가 다섯 개 있다면 아래와 같이 표현할 수 있을 것이다.\n",
    "\n",
    "#### <상태의 집합 예제 (유한한 공간인 경우)>\n",
    "\n",
    "$$\n",
    "\\pmb{S} = \\{ (x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y_5)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 2.2] 그리드월드에서 상태는 좌표를 의미**\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/02.%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%EA%B3%BC%20%EC%BC%80%EB%9D%BC%EC%8A%A4%EB%A1%9C%20%EB%B0%B0%EC%9A%B0%EB%8A%94%20%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/PNG/Fig%202.2.1.PNG?raw=true\" width=\"600\">\n",
    "\n",
    "\n",
    "에이전트는 시간에 따라 25개의 상태의 집합 안에 있는 상태를 탐험하게 됩니다. 시간은 $t$라고 표현한다. 시간 $t$일 때 상태를 $S_t$라고 표현하는데, 만약 시간이 $t$일때 상태가 $(1, 3)$이라면 다음과 같이 표현한다.\n",
    "\n",
    "#### <시간 t에 (1,3)이라는 상태>\n",
    "\n",
    "$$\n",
    "\\mathbf{S} = (1, 3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 $t$에서의 상태 $S_t$는 정해져 있지 않다! 때에 따라서 $t = 1$일때, \n",
    "\n",
    "$S_t = (1,3)$, $S_t = (4,2)$ 일 수도 있다. \n",
    "\n",
    "이처럼 어떤 집합 안에서 뽑을 때마다 달라질 수 있는 것을 **\"확률 변수 (Random Variable)\"**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <시간 t에서의 상태>\n",
    "\n",
    ": \"시간 $t$에서의 상태 $\\mathbf{S}_t$가 어떤 상태 $\\mathbf{s}$다.\"를 표현할 때 아래 수식과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{S}_t = \\mathbf{s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 행동 (Action)\n",
    "\n",
    "#### <정의> \n",
    "\n",
    "에이전트가 상태 $\\mathbf{S}_t$에서 할 수 있는 **가능한 행동의 집합**은 $\\pmb{A}$입니다.\n",
    "\n",
    "일반적으로 에이전트가 할 수 있는 행동은 모든 상태에서 같다. 따라서 하나의 집합 $\\pmb{A}$로 나타낼 수 있다.\n",
    "\n",
    "어떤 특정한 행동은 특정한 상태를 $\\mathbf{s}$로 나타내는 것과 마찬가지로 소문자 $\\mathbf{a}$로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <시간 t에서의 행동 $\\mathbf{a}$>\n",
    "\n",
    "시간 $\\mathbf{t}$에 에이전트가 특정한 행동 $\\mathbf{a}$를 했다면 아래와 같이 표현할 수 있다.\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_t = \\mathbf{a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <그리드월드에서 에이전트가 할 수 있는 행동의 집합>\n",
    "\n",
    "$$\n",
    "\\pmb{A}_t = \\{up, down, left, right\\}\n",
    "$$\n",
    "\n",
    "**[그림 2.4] 어떤 상태에서 행동을 한 후 에이전트가 이동한다.**\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/02.%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%EA%B3%BC%20%EC%BC%80%EB%9D%BC%EC%8A%A4%EB%A1%9C%20%EB%B0%B0%EC%9A%B0%EB%8A%94%20%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/PNG/Fig%202.4.PNG?raw=true\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 바람과 같은 예상치 못한 요소가 있다면 에이전트는 (4, 1)에 도달하지 못할 수 도 있습니다. 이러한 상황을 고려해주는 것이 상태 변환 확률입니다. 상태 변환 확률은 뒤에서 자세히 설명할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 보상 함수 (Reward Function)\n",
    "\n",
    "#### <정의> \n",
    "\n",
    "에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보\n",
    "\n",
    "시간 $t$에서 상태가 $\\mathbf{S}_t$ = s 이고 행동이 $\\mathbf{A}_t$일 때 에이전트가 받을 보상은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [수식 2.7] 보상함수의 정의\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_t^{a} = \\mathbf{E}[\\mathbf{R}_{t+1} | \\mathbf{S}_t = \\mathbf{s}, \\mathbf{A}_t = \\mathbf{a}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기호 $\\mathbf{E}$는 기댓값을 의미 => **평균**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <주사위의 기댓값 계산>\n",
    "\n",
    "#### [수식 2.8] 주사위의 기댓값 계산\n",
    "\n",
    "$$\n",
    "\\text{기댓값} = 1 * \\frac{1}{6} + 2 * \\frac{1}{6} + 3 * \\frac{1}{6} + 4 * \\frac{1}{6} + 5 * \\frac{1}{6} + 6 * \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "주사위를 한 번 던졌을때 나오는 숫자의 기댓값, 예상값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [수식 2.9] 기댓값의 표현\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_t^{a} = \\color{red}{\\mathbf{E}}[\\mathbf{R}_{t+1} | \\mathbf{S}_t = \\mathbf{s}, \\mathbf{A}_t = \\mathbf{a}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\mathbf{E}}$ : Expectation의 첫 글자\n",
    "- 질문 : 보상 함수는 왜 기댓값으로 표현하는 것일까? \n",
    "- 답변 : 보상을 에이전트에게 주는 것은 환경이고 환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수도 있기 때문이다. 이 모든 것을 고려하여 보상함수를 기댓값으로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [수식 2.10] 조건부 확률의 표현\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_t^{a} = \\mathbf{E}[\\mathbf{R}_{t+1} \\color{red}{|} \\mathbf{S}_t = \\mathbf{s}, \\mathbf{A}_t = \\mathbf{a}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"$\\color{red}{|}$\" : 조건문에 대한 표현, $\\color{red}{|}$를 기준으로 뒤에 나오는 부분들이 현재의 조건을 의미  \n",
    "보상함수의 조건 혹은 입력은 **상태**와 **행동**입니다.\n",
    "\n",
    "어떤 상태 $\\mathbf{s}$에서 행동 $\\mathbf{a}$를 할 때마다 받는 보상이 다를 수 있으므로 기댓값의 표현인 $\\mathbf{E}$가 붙은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [수식 2.11] `t+1`에서 받는 보상\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_t^{a} = \\mathbf{E}[\\mathbf{R}_{\\color{red}{t+1}} | \\mathbf{S}_t = \\mathbf{s}, \\mathbf{A}_t = \\mathbf{a}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보상을 에이전트가 알고 있는 것이 아니고 환경이 알려주기 때문이다.\n",
    "\n",
    "에이전트가 상태 $\\mathbf{s}$에서 행동 $\\mathbf{a}$를 하면 환경은 에이전트가 가게 되는 다음 상태 $\\mathbf{s}$와 에이전트가 받을 보상을 에이전트에게 알려준다. 환경이 에이전트에 알려주는 것이 **`t+1`**인 시점이다. 따라서 에이전트가 받는 보상을 $\\mathbf{R}_{t+1}$이라고 표현한다.\n",
    "\n",
    "**[그림 2.5] 그리드월드의 보상**\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/Deep-ReinForcement/blob/master/02.%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%EA%B3%BC%20%EC%BC%80%EB%9D%BC%EC%8A%A4%EB%A1%9C%20%EB%B0%B0%EC%9A%B0%EB%8A%94%20%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/PNG/Fig%202.5.PNG?raw=true\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
