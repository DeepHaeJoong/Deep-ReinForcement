{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥 Q 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 네트워크**는 **Q 테이블** 만큼 성능이 나오지 않았지만, **딥 Q 네트워크**는 훨씬 더 강력한 기능을 가지고 있다.\n",
    "\n",
    "**Q 네트워크**를 **DQN**으로 만들기 위해서는 다음과 같은 개선이 필요 \n",
    "\n",
    "- 단일 계층 네트워크를 다계층 합성곱 네트워크로 확장\n",
    "- 경험 리플레이의 구현, 즉 네트워크가 자신의 경험에 저장된 기억을 이용해 스스로 학습\n",
    "- 제 2의 타깃 네트워크를 활용하여 업데이트 시 타깃 Q 값을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 개선 1 : 합성곱 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 개선 2 : 경험 리플레이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Agent가 경험을 저장해두었다가 랜덤하게 경험의 일부를 뽑아서 네트워크를 학습시키는 것**\n",
    "    - 과제를 더 잘 수행할 수 있게 하여 강건한 학습이 가능 (다양한 과거 경험으로부터 학습 가능)\n",
    "    - 각 경험은 (상태, 액션, 보상, 다음 상태)와 같은 튜플로 저장\n",
    "    - 경험 리플레이 버퍼는 최근 기억 중 정해진 몇 개를 저장 (새로운 경험 추가될 때마다 정해진 경험 제거)\n",
    "    - 학습을 시작할 때가 되면 단순히 버퍼에서 랜덤한 기억 더미를 뽑아 네트워크를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 개선 3 : 별도의 타깃 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**제 2의 네트워크, 타깃 네트워크를 활용**\n",
    "- 학습 시 모든 액션에 대한 비용을 계산하기 위해 이용되는 타깃 Q 값을 생성한다.\n",
    "- 질문 : 왜 하나의 네트워크를 사용하지 않는 것일까?\n",
    "- 답 : 학습의 각 단계에서 Q 네트워크의 값은 변화하므로, 이 일련의 변화하는 값을 네트워크 값을 조절하는데에 이용하면 값을 추정하는 것이 통제 불능 상태에 빠지기 쉽다. 즉, 네트워크가 타깃 Q 값과 예측 Q값 간의 피드백 루프에 빠지면서 불안정해질 수 있다.\n",
    "- 해결 방안 : 타깃 네트워크의 가중치는 고정, Q 네트워크 값은 주기적으로 또는 천천히 업데이트 시 학습을 안정적으로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 DQN을 넘어서 (2015년 기술)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더블 DQN + 듀얼링 DQN = 더 빠른 학습 시간 & 더 나은 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 더블 DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Double DQN 뒤에 있는 주요 통찰은 평범한 DQN이 자주 주어진 상태에서 취할 수 있는 잠재적 행동의 Q-value를 과대평가한다는 것이다. 만약 모든 행동이 항상 동등하게 과대평가된다면, 괜찮겠지만, 이것이 사실이 아니라는 것을 믿을만한 증거가 있다. 쉽게 상상할 수 있다. 만약 특정 차선의 행동이 규칙적으로 최선의 행동보다 높은 Q-value를 갖는다면, 에이전트는 이상적인 정책을 학습하는데 어려움이 있다. 이것을 고치기 위해서, DDQN 논문의 저자는 단순한 트릭을 제안했다. 우리의 학습 단계에서 target Q vlaue를 계산할 때 Q value 들 중 최대값을 선택하는 것 대신에 우리는 우리의 주요 신경망이 행동을 선택하게 하고, 우리의 타겟 신경망은 행동에 대한 target Q value을 생성하게 하는 것을 사용한다. 행동이 target Q-value 생성으로부터 행동 선택을 분리함으로써, 우리는 대체로 과대평가를 줄일 수 있고, 학습을 빠르게 하고 더 믿을 수 있다. 아래는 target value를 업데이트 하는 것에 대한 새로운 DDQN 방정식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_{target} = \\gamma + \\eta Q(s^{'} , argmax(Q(s^{'},\\alpha,\\theta ), \\theta^{'}))\n",
    "$$\n",
    "\n",
    "- $Q_{target}$ : 선택된 액션에 대한 우리의 타깃 Q값\n",
    "- $Q_{current}$ : 현재 예측한 Q값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 듀얼링 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
